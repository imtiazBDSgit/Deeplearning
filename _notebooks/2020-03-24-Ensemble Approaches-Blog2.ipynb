{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ensembling Approaches\n",
    "> A tutorial of ensembling approaches.\n",
    "> Disclaimer : Everything present in this notebook has been taken from blogs over the internet.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope of ensemble approaches covered\n",
    "\n",
    "Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it. Some of the ensembles covered here are below.\n",
    "\n",
    "   - Majority Voting\n",
    "   - Majority Weighted Voting\n",
    "   - Simple Average\n",
    "   - Weighted Average\n",
    "   - Stacking Variant A\n",
    "   - Stacking Variant B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide1.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide2.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from vecstack import StackingTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide3.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier\n",
    "- In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.\n",
    "\n",
    "- E.g., if the prediction for a given sample is\n",
    "\n",
    "    - classifier 1 -> class 1\n",
    "\n",
    "    - classifier 2 -> class 1\n",
    "\n",
    "    - classifier 3 -> class 2\n",
    "\n",
    "    - the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label.\n",
    "\n",
    "- In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario\n",
    "\n",
    "    - classifier 1 -> class 2\n",
    "\n",
    "    - classifier 2 -> class 1\n",
    "\n",
    "    - the class label 1 will be assigned to the sample.\n",
    "\n",
    "- In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted  probabilities.\n",
    "\n",
    "- Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. \n",
    "- The final class label is then derived from the class label with the highest average probability.\n",
    "\n",
    "\n",
    "| Classifier  | class1 | class 2 | class 3 |\n",
    "|------------ |--------|---------|---------|\n",
    "| Classifier1 |0.2*w1  |0.5*w1   |0.3*w1   |\n",
    "| Classifier2 |0.6*w2  |0.3*w2   |0.1*w2   |\n",
    "| Classifier3 |0.3*w3  |0.4*w3   |0.3*w3   |\n",
    "| weighted Avg|0.37    |0.4      |0.23     |\n",
    "\n",
    "- Here, the predicted class label is 2, since it has the highest average probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide4.GIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def votingClassifier(models_dict,voting='hard',weights=None):\n",
    "    estimators=[(modelName,model) for modelName,model in models_dict.items()]\n",
    "    if weights:\n",
    "        model = VotingClassifier(estimators=estimators, voting='hard')\n",
    "    else:\n",
    "        model = VotingClassifier(estimators=estimators, voting='soft',weights=weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide5.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average\n",
    "\n",
    "- Similar to the max voting technique, multiple predictions are made for each data point in averaging. \n",
    "- In this method, we take an average of predictions from all the models and use it to make the final prediction.\n",
    "- Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide6.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average\n",
    "- This is an extension of the averaging method.\n",
    "- All models are assigned different weights defining the importance of each model for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide7.GIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageWeightClassifier(predModels,weights=None):\n",
    "    if weights is None:\n",
    "        weights=[1/len(predModels) for _ in range(len(predModels))]\n",
    "    return [sum(l*weights) for l in zip(*predModels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "\n",
    "- Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. \n",
    "- Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n",
    "\n",
    "- The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm.\n",
    "\n",
    "\n",
    "#### Stacking Concept\n",
    "\n",
    "1. We want to predict train set and test set with some 1st level model(s), and then use these predictions as features for 2nd level model(s).\n",
    "2. Any model can be used as 1st level model or 2nd level model.\n",
    "3. To avoid overfitting (for train set) we use cross-validation technique and in each fold we predict out-of-fold (OOF) part of train set.\n",
    "4. The common practice is to use from 3 to 10 folds.\n",
    "\n",
    "###### Predict test set:\n",
    "\n",
    "- Variant A: In each fold we predict test set, so after completion of all folds we need to find mean (mode) of all temporary test set predictions made in each fold.\n",
    "\n",
    "![](my_icons/Slide8.GIF)\n",
    "\n",
    "- Variant B: We do not predict test set during cross-validation cycle. After completion of all folds we perform additional step: fit model on full train set and predict test set once. This approach takes more time because we need to perform one additional fitting.\n",
    "\n",
    "![](my_icons/Slide9.GIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution! All estimators and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended.\n",
    "# This is list of tuples\n",
    "# Each tuple contains arbitrary unique name and estimator object\n",
    "estimators_L1 = [\n",
    "    ('et', ExtraTreesRegressor(random_state=0, n_jobs=-1, \n",
    "                               n_estimators=100, max_depth=3)),\n",
    "        \n",
    "    ('rf', RandomForestRegressor(random_state=0, n_jobs=-1, \n",
    "                                 n_estimators=100, max_depth=3)),\n",
    "        \n",
    "    ('xgb', XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                         n_estimators=100, max_depth=3))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize stacking transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingTransformer(estimators=estimators_L1,   # base estimators\n",
    "                            regression=False,           # regression task (if you need \n",
    "                                                        # classification - set to False)\n",
    "                            variant='A',                # oof for train set, predict test \n",
    "                                                        # set in each fold and find mean\n",
    "                            metric=log_loss,            # metric: callable\n",
    "                            n_folds=4,                  # number of folds\n",
    "                            shuffle=True,               # shuffle the data\n",
    "                            random_state=0,             # ensure reproducibility\n",
    "                            verbose=2,                  # print all info\n",
    "                            needs_proba=True)           # gives probability scores.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train = stack.transform(X_train)\n",
    "S_test = stack.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 2nd level estimator\n",
    "final_estimator = XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                               n_estimators=100, max_depth=3)\n",
    "\n",
    "# Fit\n",
    "final_estimator = final_estimator.fit(S_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = final_estimator.predict(S_test)\n",
    "\n",
    "# Final prediction score\n",
    "print('Final prediction score: [%.8f]' % log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of base estimators\n",
    "# Type: int\n",
    "stack.n_estimators_\n",
    "\n",
    "# Scores for each estimator (rows) in each fold (columns)\n",
    "# Type: 2d numpy array\n",
    "stack.scores_\n",
    "\n",
    "# Mean and std for each estimator\n",
    "# Type: list of tuples\n",
    "stack.mean_std_\n",
    "\n",
    "# Mean and std convenient representation using pandas.DataFrame\n",
    "df = pd.DataFrame.from_records(stack.mean_std_, columns=['name', 'mean', 'std'])\n",
    "# Sort by column 'mean' (best on the top)\n",
    "df.sort_values('mean', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/Slide10.GIF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
