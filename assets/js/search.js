
var documents = [{
    "id": 0,
    "url": "https://imtiazBDSgit.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://imtiazBDSgit.github.io/about/",
    "title": "About Me",
    "body": "Learning to Learn things , Let me know if my learnings are useful to you. Regards,Imtiaz "
    }, {
    "id": 2,
    "url": "https://imtiazBDSgit.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://imtiazBDSgit.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://imtiazBDSgit.github.io/2020/03/24/Ensembling-Approaches.html",
    "title": "Machine Learning Ensembling Approaches",
    "body": "2020/03/24 -           Scope of ensemble approaches covered : Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it. Some of the ensembles covered here are below. Majority VotingMajority Weighted VotingSimple AverageWeighted AverageStacking Variant AStacking Variant B Packages to import :       from sklearn. ensemble import VotingClassifierimport pandas as pdfrom sklearn. datasets import load_bostonfrom sklearn. model_selection import train_test_splitfrom sklearn. metrics import log_lossfrom sklearn. ensemble import ExtraTreesRegressorfrom sklearn. ensemble import RandomForestRegressorfrom sklearn. pipeline import Pipelinefrom sklearn. externals import joblibfrom xgboost import XGBRegressorfrom vecstack import StackingTransformer     Voting Classifier : In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. E. g. , if the prediction for a given sample is classifier 1 -&gt; class 1 classifier 2 -&gt; class 1 classifier 3 -&gt; class 2 the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E. g. , in the following scenario classifier 1 -&gt; class 2 classifier 2 -&gt; class 1 the class label 1 will be assigned to the sample. In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. Classifierclass1class 2class 3Classifier10. 2*w10. 5*w10. 3*w1Classifier20. 6*w20. 3*w20. 1*w2Classifier30. 3*w30. 4*w30. 3*w3weighted Avg0. 370. 40. 23Here, the predicted class label is 2, since it has the highest average probability.       def votingClassifier(models_dict,voting=&#39;hard&#39;,weights=None):  estimators=[(modelName,model) for modelName,model in models_dict. items()]  if weights:    model = VotingClassifier(estimators=estimators, voting=&#39;hard&#39;)  else:    model = VotingClassifier(estimators=estimators, voting=&#39;soft&#39;,weights=weights)  return model     Average : Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems. Weighted Average : This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.       def AverageWeightClassifier(predModels,weights=None):  if weights is None:    weights=[1/len(predModels) for _ in range(len(predModels))]  return [sum(l*weights) for l in zip(*predModels)]    Stacking : Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm. Stacking Concept : We want to predict train set and test set with some 1st level model(s), and then use these predictions as features for 2nd level model(s). Any model can be used as 1st level model or 2nd level model. To avoid overfitting (for train set) we use cross-validation technique and in each fold we predict out-of-fold (OOF) part of train set. The common practice is to use from 3 to 10 folds. Predict test set: Variant A: In each fold we predict test set, so after completion of all folds we need to find mean (mode) of all temporary test set predictions made in each fold. Variant B: We do not predict test set during cross-validation cycle. After completion of all folds we perform additional step: fit model on full train set and predict test set once. This approach takes more time because we need to perform one additional fitting.       # Caution! All estimators and parameter values are just # demonstrational and shouldn&#39;t be considered as recommended. # This is list of tuples# Each tuple contains arbitrary unique name and estimator objectestimators_L1 = [  (&#39;et&#39;, ExtraTreesRegressor(random_state=0, n_jobs=-1,                n_estimators=100, max_depth=3)),      (&#39;rf&#39;, RandomForestRegressor(random_state=0, n_jobs=-1,                 n_estimators=100, max_depth=3)),      (&#39;xgb&#39;, XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0. 1,             n_estimators=100, max_depth=3))]    Initialize stacking transformer :       stack = StackingTransformer(estimators=estimators_L1,  # base estimators              regression=False,      # regression task (if you need                             # classification - set to False)              variant=&#39;A&#39;,        # oof for train set, predict test                             # set in each fold and find mean              metric=log_loss,      # metric: callable              n_folds=4,         # number of folds              shuffle=True,        # shuffle the data              random_state=0,       # ensure reproducibility              verbose=2,         # print all info              needs_proba=True)      # gives probability scores.              stack = stack. fit(X_train, y_train)          S_train = stack. transform(X_train)S_test = stack. transform(X_test)          # Initialize 2nd level estimatorfinal_estimator = XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0. 1,                n_estimators=100, max_depth=3)# Fitfinal_estimator = final_estimator. fit(S_train, y_train)# Predicty_pred = final_estimator. predict(S_test)# Final prediction scoreprint(&#39;Final prediction score: [%. 8f]&#39; % log_loss(y_test, y_pred))          # Number of base estimators# Type: intstack. n_estimators_# Scores for each estimator (rows) in each fold (columns)# Type: 2d numpy arraystack. scores_# Mean and std for each estimator# Type: list of tuplesstack. mean_std_# Mean and std convenient representation using pandas. DataFramedf = pd. DataFrame. from_records(stack. mean_std_, columns=[&#39;name&#39;, &#39;mean&#39;, &#39;std&#39;])# Sort by column &#39;mean&#39; (best on the top)df. sort_values(&#39;mean&#39;, ascending=True)     "
    }, {
    "id": 5,
    "url": "https://imtiazBDSgit.github.io/2020/03/24/Ensemble-Approaches-Blog2.html",
    "title": "Machine Learning Ensembling Approaches",
    "body": "2020/03/24 -           Scope of ensemble approaches covered : Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it. Some of the ensembles covered here are below. Majority VotingMajority Weighted VotingSimple AverageWeighted AverageStacking Variant AStacking Variant B Packages to import :       from sklearn. ensemble import VotingClassifierimport pandas as pdfrom sklearn. datasets import load_bostonfrom sklearn. model_selection import train_test_splitfrom sklearn. metrics import log_lossfrom sklearn. ensemble import ExtraTreesRegressorfrom sklearn. ensemble import RandomForestRegressorfrom sklearn. pipeline import Pipelinefrom sklearn. externals import joblibfrom xgboost import XGBRegressorfrom vecstack import StackingTransformer     Voting Classifier : In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. E. g. , if the prediction for a given sample is classifier 1 -&gt; class 1 classifier 2 -&gt; class 1 classifier 3 -&gt; class 2 the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E. g. , in the following scenario classifier 1 -&gt; class 2 classifier 2 -&gt; class 1 the class label 1 will be assigned to the sample. In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. Classifierclass1class 2class 3Classifier10. 2*w10. 5*w10. 3*w1Classifier20. 6*w20. 3*w20. 1*w2Classifier30. 3*w30. 4*w30. 3*w3weighted Avg0. 370. 40. 23Here, the predicted class label is 2, since it has the highest average probability.       def votingClassifier(models_dict,voting=&#39;hard&#39;,weights=None):  estimators=[(modelName,model) for modelName,model in models_dict. items()]  if weights:    model = VotingClassifier(estimators=estimators, voting=&#39;hard&#39;)  else:    model = VotingClassifier(estimators=estimators, voting=&#39;soft&#39;,weights=weights)  return model     Average : Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems. Weighted Average : This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.       def AverageWeightClassifier(predModels,weights=None):  if weights is None:    weights=[1/len(predModels) for _ in range(len(predModels))]  return [sum(l*weights) for l in zip(*predModels)]    Stacking : Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm. Stacking Concept : We want to predict train set and test set with some 1st level model(s), and then use these predictions as features for 2nd level model(s). Any model can be used as 1st level model or 2nd level model. To avoid overfitting (for train set) we use cross-validation technique and in each fold we predict out-of-fold (OOF) part of train set. The common practice is to use from 3 to 10 folds. Predict test set: Variant A: In each fold we predict test set, so after completion of all folds we need to find mean (mode) of all temporary test set predictions made in each fold. Variant B: We do not predict test set during cross-validation cycle. After completion of all folds we perform additional step: fit model on full train set and predict test set once. This approach takes more time because we need to perform one additional fitting.       # Caution! All estimators and parameter values are just # demonstrational and shouldn&#39;t be considered as recommended. # This is list of tuples# Each tuple contains arbitrary unique name and estimator objectestimators_L1 = [  (&#39;et&#39;, ExtraTreesRegressor(random_state=0, n_jobs=-1,                n_estimators=100, max_depth=3)),      (&#39;rf&#39;, RandomForestRegressor(random_state=0, n_jobs=-1,                 n_estimators=100, max_depth=3)),      (&#39;xgb&#39;, XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0. 1,             n_estimators=100, max_depth=3))]    Initialize stacking transformer :       stack = StackingTransformer(estimators=estimators_L1,  # base estimators              regression=False,      # regression task (if you need                             # classification - set to False)              variant=&#39;A&#39;,        # oof for train set, predict test                             # set in each fold and find mean              metric=log_loss,      # metric: callable              n_folds=4,         # number of folds              shuffle=True,        # shuffle the data              random_state=0,       # ensure reproducibility              verbose=2,         # print all info              needs_proba=True)      # gives probability scores.              stack = stack. fit(X_train, y_train)          S_train = stack. transform(X_train)S_test = stack. transform(X_test)          # Initialize 2nd level estimatorfinal_estimator = XGBRegressor(random_state=0, n_jobs=-1, learning_rate=0. 1,                n_estimators=100, max_depth=3)# Fitfinal_estimator = final_estimator. fit(S_train, y_train)# Predicty_pred = final_estimator. predict(S_test)# Final prediction scoreprint(&#39;Final prediction score: [%. 8f]&#39; % log_loss(y_test, y_pred))          # Number of base estimators# Type: intstack. n_estimators_# Scores for each estimator (rows) in each fold (columns)# Type: 2d numpy arraystack. scores_# Mean and std for each estimator# Type: list of tuplesstack. mean_std_# Mean and std convenient representation using pandas. DataFramedf = pd. DataFrame. from_records(stack. mean_std_, columns=[&#39;name&#39;, &#39;mean&#39;, &#39;std&#39;])# Sort by column &#39;mean&#39; (best on the top)df. sort_values(&#39;mean&#39;, ascending=True)     "
    }, {
    "id": 6,
    "url": "https://imtiazBDSgit.github.io/jupyter/2020/02/20/test.html",
    "title": "Fastpages Notebook Blog Post",
    "body": "2020/02/20 -           About This notebook is a demonstration of some of capabilities of fastpages with notebooks. With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! Front Matter : Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: Setting toc: true will automatically generate a table of contentsSetting badges: true will automatically include GitHub and Google Colab links to your notebook. Setting comments: true will enable commenting on your blog post, powered by utterances. More details and options for front matter can be viewed on the front matter section of the README. Markdown Shortcuts : put a #hide flag at the top of any cell you want to completely hide in the docs put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:              #collapse-hideimport pandas as pdimport altair as alt       put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:              #collapse-showcars = &#39;https://vega. github. io/vega-datasets/data/cars. json&#39;movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;sp500 = &#39;https://vega. github. io/vega-datasets/data/sp500. csv&#39;stocks = &#39;https://vega. github. io/vega-datasets/data/stocks. csv&#39;flights = &#39;https://vega. github. io/vega-datasets/data/flights-5k. json&#39;       Interactive Charts With Altair : Charts made with Altair remain interactive.  Example charts taken from this repo, specifically this notebook. Example 1: DropDown :       # single-value selection over [Major_Genre, MPAA_Rating] pairs# use specific hard-wired values as the initial selected valuesselection = alt. selection_single(  name=&#39;Select&#39;,  fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;],  init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;},  bind={&#39;Major_Genre&#39;: alt. binding_select(options=genres), &#39;MPAA_Rating&#39;: alt. binding_radio(options=mpaa)}) # scatter plot, modify opacity based on selectionalt. Chart(movies). mark_circle(). add_selection(  selection). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=&#39;IMDB_Rating:Q&#39;,  tooltip=&#39;Title:N&#39;,  opacity=alt. condition(selection, alt. value(0. 75), alt. value(0. 05)))    Example 2: Tooltips :       alt. Chart(movies). mark_circle(). add_selection(  alt. selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;])). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=alt. Y(&#39;IMDB_Rating:Q&#39;, axis=alt. Axis(minExtent=30)), # use min extent to stabilize axis title placement  tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;]). properties(  width=600,  height=400)    Example 3: More Tooltips :       # select a point for which to provide details-on-demandlabel = alt. selection_single(  encodings=[&#39;x&#39;], # limit selection to x-axis value  on=&#39;mouseover&#39;, # select on mouseover events  nearest=True,  # select data point nearest the cursor  empty=&#39;none&#39;   # empty selection includes no data points)# define our base line chart of stock pricesbase = alt. Chart(). mark_line(). encode(  alt. X(&#39;date:T&#39;),  alt. Y(&#39;price:Q&#39;, scale=alt. Scale(type=&#39;log&#39;)),  alt. Color(&#39;symbol:N&#39;))alt. layer(  base, # base line chart    # add a rule mark to serve as a guide line  alt. Chart(). mark_rule(color=&#39;#aaa&#39;). encode(    x=&#39;date:T&#39;  ). transform_filter(label),    # add circle marks for selected time points, hide unselected points  base. mark_circle(). encode(    opacity=alt. condition(label, alt. value(1), alt. value(0))  ). add_selection(label),  # add white stroked text to provide a legible background for labels  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),  # add text labels for stock prices  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),    data=stocks). properties(  width=700,  height=400)    Data Tables : You can display tables per the usual way in your blog:       movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;df = pd. read_json(movies)# display table with pandasdf[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;,   &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]]. head()           Title   Worldwide_Gross   Production_Budget   IMDB_Rating         0   The Land Girls   146083. 0   8000000. 0   6. 1       1   First Love, Last Rites   10876. 0   300000. 0   6. 9       2   I Married a Strange Person   203134. 0   250000. 0   6. 8       3   Let's Talk About Sex   373615. 0   300000. 0   NaN       4   Slam   1087521. 0   1000000. 0   3. 4     Images : Local Images : You can reference local images and they will be copied and rendered on your blog automatically.  You can include these with the following markdown syntax: ![](my_icons/fastai_logo. png) Remote Images : Remote images can be included with the following markdown syntax: ![](https://image. flaticon. com/icons/svg/36/36686. svg) Animated Gifs : Animated Gifs work, too! ![](https://upload. wikimedia. org/wikipedia/commons/7/71/ChessPawnSpecialMoves. gif) Captions : You can include captions with markdown images like this: ![](https://www. fast. ai/images/fastai_paper/show_batch. png  Credit: https://www. fast. ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ ) Other Elements Tweetcards : Typing &gt; twitter: https://twitter. com/jakevdp/status/1204765621767901185?s=20 will render this:Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Youtube Videos : Typing &gt; youtube: https://youtu. be/XfoYk_Z5AkI will render this: Boxes / Callouts : Typing &gt; Warning: There will be no second warning! will render this:    Warning: There will be no second warning! Typing &gt; Important: Pay attention! It's important. will render this:    Important: Pay attention! It&#8217;s important. Typing &gt; Tip: This is my tip. will render this:    Tip: This is my tip. Typing &gt; Note: Take note of this. will render this:    Note: Take note of this. Typing &gt; Note: A doc link to [an example website: fast. ai](https://www. fast. ai/) should also work fine. will render in the docs:    Note: A doc link to an example website: fast. ai should also work fine. Footnotes : You can have footnotes in notebooks just like you can with markdown. For example, here is a footnote 1. This is the footnote. &#8617; "
    }, {
    "id": 7,
    "url": "https://imtiazBDSgit.github.io/markdown/2020/01/14/test-markdown-post.html",
    "title": "Example Markdown Post",
    "body": "2020/01/14 - Basic setup: Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-filename. md Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. . md is the file extension for markdown files. The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. Basic formatting: You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: Lists: Here’s a list:  item 1 item 2And a numbered list:  item 1 item 2Boxes and stuff:  This is a quotation    You can include alert boxes…and…    You can include info boxesImages: Code: General preformatted text: # Do a thingdo_thing()Python code and output: # Prints '2'print(1+1)2Tables:       Column 1   Column 2         A thing   Another thing   Tweetcards: Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019Footnotes:       This is the footnote.  &#8617;    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}